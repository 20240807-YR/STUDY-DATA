# STUDY-DATA
## 📅 12월 2–3일 작업 요약
## 📌 12월 2–3일: 네이버 뷰티 검색 기록 분석 / 쇼핑 데이터 수집 / 인구·GRDP 분석 / 감성 분석 기초 모델 준비

⸻

### 12_03_네이버뷰티검색기록.ipynb — 네이버 뷰티 검색 기록 분석

* **데이터 로드 및 전처리**
	•	네이버뷰티검색기록.csv 로드 후 헤더 시작 위치를 탐색하기 위해 날짜 정규식(\d{4}-\d{2}-\d{2})으로 실제 데이터 시작행(start_idx) 자동 탐지.
	•	skiprows=6 적용하여 본 데이터 프레임 재로드.
	•	날짜 컬럼 → datetime 변환, 스킨케어·트러블케어·선케어·클렌징·메이크업 등 주요 카테고리만 추출해 clean_df 생성.
	•	index를 날짜로 설정하고 숫자형으로 정리.

* **검색량 추세 분석**
	•	AppleSDGothic 폰트 적용 후 카테고리별 검색량 추세 그래프 시각화.

* **증감률 분석**
	•	일별 증감률(pct_change), 전년 대비 증가율(yoy), 월별 평균 증가율(MoM) 계산.
	•	0만 포함하는 행 제거 후 재정제.

* **시즌성 분석**
	•	카테고리별 최고 관심도 시점(peak) / 최저 관심도 시점(low) 산출.

* **상관 분석**
	•	카테고리 간 상관행렬 계산 후 heatmap 시각화.

* **요일 패턴 분석**
	•	weekday 컬럼 생성 → 요일별 평균 검색량 집계 및 bar plot 시각화.

⸻

### 12_03_쇼핑.ipynb — 네이버 쇼핑인사이트 스킨케어 데이터 수집(크롤링)

**네이버 쇼핑 Insight 자동 다운로드**
	•	selenium + ChromeDriverManager 사용해 브라우저 자동 실행.
	•	카테고리 선택:
	•	1차: 화장품/미용
	•	2차: 스킨케어
	•	“데이터 다운로드” 버튼 클릭 → 쇼핑 검색량 CSV 자동 다운로드.

⸻

### 12_03_인구데이터성별.ipynb — 20~34세 여성 인구 분석

* **데이터 로드 및 기본 전처리**
	•	인구데이터성별.csv 로드 → 컬럼명(region, sex, age, pop) 정리.
	•	“연령별”에서 “20~34세 여성”만 추출하여 분석.

* **주요 처리**
	•	20~34세 여성 전체 인구 합계 계산.
	•	지역별 여성 20~34세 인구 Top10 도출 → 반올림 처리.
	•	수도권(서울·경기·인천) 집중도 계산.
	•	지역 전체 인구 대비 20–34세 여성 비중 산출.
	•	연령대(20–24 / 25–29 / 30–34) 구간별 지역 분포 테이블 생성.
	•	bar chart로 Top10 시각화.

⸻

### 12_03_인구통계.ipynb — 장기 시계열 기반 여성2034 인구 분석

* **데이터 로드 및 정리**
	•	인구통계.csv 로드, 컬럼 재정의(region, age, sex, time, population).
	•	latest = df["time"].max() 로 최신 시점 추출.

* **주요 분석** 
	•	최신 시점 기준 20~34세 여성 데이터 추출.
	•	지역별 인구 합산 후 Top10 도출.
	•	전체 데이터 개수 / 여성 데이터 개수 / 최신 시점 데이터 개수 등 기본 검증 출력.

⸻

### 12_03_GRDP.ipynb — GRDP·소득·소비 분석 및 클러스터링

* **데이터 전처리**
	•	GRDP시도별.csv 로드 → GRDP, 소득, 민간소비 컬럼 정리.
	•	수치형 변환.

* **주요 분석**
	•	GRDP 상위 TOP10 시각화(bar).
	•	민간소비 / 개인소득 비율(consumption_ratio) 계산.
	•	소비 성향 상위 Top10 시각화.
	•	소비 성향을 4개 클러스터로 분류(보수적/평균/적극적/초고소비).

* Beauty Market Potential Score 기초 구상
	•	Potential = (2034 여성 인구 비중 × 0.5) + (소비성향 × 0.3) + (GRDP × 0.2)

⸻

### 12_02_sentiment.ipynb — 감성 분석 기초 모델 구축(고객 CRM 메시지 생성용)

* **데이터 로드 및 정리**
	•	테스트용.csv 로드(latin-1 인코딩).
	•	감성 레이블(0 부정 / 4 긍정 / 2 중립) 구조 이해.
	•	불필요한 flag 컬럼 drop → df_model = [label, text] 구성.

* **텍스트 전처리**
	•	nltk stopwords 다운로드.
	•	기본 전처리:
	•	URL 제거
	•	@유저 제거
	•	알파벳·공백 제외 정리
	•	custom stopwords(im, ive, dont, rt, amp …) 추가.

* **부정(label=0) 텍스트 단어 분석**
	•	tqdm 기반 전체 토큰 분석 → Counter로 단어 frequency 계산.
	•	최종 전처리 함수 확장 및 token 확인(clean_text("RT …")).
